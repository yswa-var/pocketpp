import logging
import requests
from newspaper import Article as NewsArticle
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from requests.exceptions import RequestException
import time
import random

logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# List of user-agents to rotate
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:130.0) Gecko/20100101 Firefox/130.0",
]


def scrape_and_summarize(url, max_retries=3):
    logger.info(f"Starting to scrape URL: {url}")
    for attempt in range(max_retries):
        try:
            # Primary method: newspaper3k
            logger.info("Attempting to scrape with newspaper3k")
            article = NewsArticle(url)
            article.download()
            logger.info("Article downloaded successfully")
            article.parse()
            logger.info("Article parsed successfully")
            article.nlp()
            logger.info("Article NLP processed successfully")

            data = {
                "title": article.title,
                "content": article.text,
                "summary": article.summary,
                "image_url": article.top_image,
                "url": url,
                "category": "General"  # Default category
            }
            logger.info(f"Scraped data: Title={data['title']}")
            return enhance_data_with_gemma(data, url)
        except Exception as e:
            logger.warning(
                f"Primary method failed (attempt {attempt + 1}): {e}")

        try:
            # Fallback 1: requests + BeautifulSoup
            logger.info("Falling back to requests + BeautifulSoup")
            data = scrape_with_requests(url)
            if data["title"] and data["content"]:
                logger.info("Successfully scraped with requests")
                return enhance_data_with_gemma(data, url)
            logger.warning("Requests scraping returned insufficient data.")
        except Exception as e:
            logger.warning(
                f"Requests fallback failed (attempt {attempt + 1}): {e}")

        try:
            # Fallback 2: Selenium
            logger.info("Falling back to Selenium")
            data = scrape_with_selenium(url)
            if data["title"] and data["content"]:
                logger.info("Successfully scraped with Selenium")
                return enhance_data_with_gemma(data, url)
            logger.warning("Selenium scraping returned insufficient data.")
        except Exception as e:
            logger.warning(
                f"Selenium fallback failed (attempt {attempt + 1}): {e}")

        if attempt < max_retries - 1:
            logger.info(f"Retrying... ({attempt + 2}/{max_retries})")
            # Random delay to avoid rate-limiting
            time.sleep(random.uniform(1, 3))

    # Final fallback: minimal data
    logger.error("All scraping attempts failed. Returning minimal data.")
    return {
        "title": "Unknown",
        "content": "",
        "summary": "Could not scrape content.",
        "image_url": "",
        "url": url,
        "category": "General"
    }


def enhance_data_with_gemma(data, url):
    logger.info("Enhancing data with Gemma")
    try:
        summary = get_gemma_summary(data["content"])
        logger.info("Summary generated by Gemma")
    except Exception as e:
        logger.warning(f"Gemma summary failed: {e}. Using placeholder.")
        summary = data["content"][:200] + \
            "..." if data["content"] else "No summary available."

    try:
        category = get_gemma_category(data["content"])
        logger.info(f"Category suggested by Gemma: {category}")
    except Exception as e:
        logger.warning(
            f"Gemma categorization failed: {e}. Defaulting to 'General'.")
        category = "General"

    data.update({
        "summary": summary,
        "category": category
    })
    return data


def get_gemma_summary(text):
    logger.info("Requesting summary from Gemma")
    try:
        # Truncate for API
        prompt = f"Summarize this article in 50-100 words:\n\n{text[:2000]}"
        response = requests.post(
            "http://localhost:1234/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": "gemma-3-4b-it",
                "messages": [
                    {"role": "system", "content": "You are a helpful assistant that summarizes articles concisely."},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.7,
                "max_tokens": 150,
                "stream": False
            }
        )
        response.raise_for_status()
        summary = response.json()["choices"][0]["message"]["content"].strip()
        logger.info(f"Gemma summary: {summary}")
        return summary
    except Exception as e:
        logger.error(
            f"Error generating summary with Gemma: {e}", exc_info=True)
        raise


def get_gemma_category(text):
    logger.info("Requesting category from Gemma")
    try:
        # Truncate for API
        prompt = f"Based on this article, suggest a single category (e.g., History, Politics, Science):\n\n{text[:2000]}"
        response = requests.post(
            "http://localhost:1234/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": "gemma-3-4b-it",
                "messages": [
                    {"role": "system", "content": "You are a helpful assistant that categorizes articles accurately reply in 2 words only reply with the catogary only."},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.7,
                "max_tokens": 50,
                "stream": False
            }
        )
        response.raise_for_status()
        category = response.json()["choices"][0]["message"]["content"].strip()
        logger.info(f"Gemma category: {category}")
        return category
    except Exception as e:
        logger.error(
            f"Error generating category with Gemma: {e}", exc_info=True)
        raise


def scrape_with_requests(url):
    logger.info("Attempting to scrape with requests")
    headers = {"User-Agent": random.choice(USER_AGENTS)}
    response = requests.get(url, headers=headers, timeout=10)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")

    # Extract title
    title = soup.find("title") or soup.find("h1")
    title = title.get_text().strip() if title else "Unknown"

    # Extract content (try common tags)
    content = ""
    for tag in ["article", "main", "div[class*='content']", "div[class*='article']"]:
        section = soup.select_one(tag)
        if section:
            content = " ".join(p.get_text().strip()
                               for p in section.find_all("p"))
            if content:
                break
    if not content:
        # Fallback to all paragraphs
        content = " ".join(p.get_text().strip() for p in soup.find_all("p"))

    # Extract image
    image_url = ""
    img = soup.find(
        "img", {"class": ["hero", "featured", "thumbnail"]}) or soup.find("img")
    if img and img.get("src"):
        image_url = img["src"]
        if not image_url.startswith("http"):
            image_url = requests.compat.urljoin(url, image_url)

    return {
        "title": title,
        "content": content,
        "image_url": image_url,
        "url": url
    }


def scrape_with_selenium(url):
    logger.info("Attempting to scrape with Selenium")
    options = Options()
    options.add_argument("--headless")
    options.add_argument(f"user-agent={random.choice(USER_AGENTS)}")
    driver = webdriver.Chrome(options=options)
    try:
        driver.get(url)
        time.sleep(3)  # Wait for JavaScript to load
        soup = BeautifulSoup(driver.page_source, "html.parser")

        # Extract title
        title = soup.find("title") or soup.find("h1")
        title = title.get_text().strip() if title else "Unknown"

        # Extract content
        content = ""
        for tag in ["article", "main", "div[class*='content']", "div[class*='article']"]:
            section = soup.select_one(tag)
            if section:
                content = " ".join(p.get_text().strip()
                                   for p in section.find_all("p"))
                if content:
                    break
        if not content:
            content = " ".join(p.get_text().strip()
                               for p in soup.find_all("p"))

        # Extract image
        image_url = ""
        img = soup.find(
            "img", {"class": ["hero", "featured", "thumbnail"]}) or soup.find("img")
        if img and img.get("src"):
            image_url = img["src"]
            if not image_url.startswith("http"):
                image_url = requests.compat.urljoin(url, image_url)

        return {
            "title": title,
            "content": content,
            "image_url": image_url,
            "url": url
        }
    finally:
        driver.quit()
